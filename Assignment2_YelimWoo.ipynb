{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yelimwoo/IS883_Azure_OpenAI_demo/blob/main/Assignment2_YelimWoo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGwQeMHQqY_3"
      },
      "source": [
        "#**Using Transformers for language modeling**\n",
        "\n",
        "In this assignment, you will experiment with using transformers to solve two different language modeling roblems: Text generation and translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUd9MZg95E79"
      },
      "source": [
        "- Some packages you may need. You are free to use alternative ones, but this should make your task simpler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUKJ_GcK5Ela"
      },
      "outputs": [],
      "source": [
        "# You only need to run this once when you load the notebook to install required packages. You can comment this cell out once you run it.\n",
        "\n",
        "# !pip install torch\n",
        "#!pip install datasets\n",
        "#!pip install apache_beam mwparserfromhell\n",
        "#!pip install transformers[torch]\n",
        "#!pip install sentence_transformers\n",
        "#!pip install evaluate\n",
        "#!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Connect to Google Drive"
      ],
      "metadata": {
        "id": "3a33z_MdOdvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qEyFXeXQOWP7",
        "outputId": "72289c5b-c177-4cc3-f6d5-19bdbbbe8469",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8CUJw2KAMSC"
      },
      "source": [
        "- Check if GPU is available. If so, it should print `cuda`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmBx6OTIAJFb",
        "outputId": "28c022fb-62f3-47c5-e1fe-acbc8bd100f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOEHB0JR1wyc"
      },
      "source": [
        "##**Part 1: Using a Transformer to model Wikipedia text**\n",
        "\n",
        "You will use a GPT2 Transformer to model the data [simple Wikipedia dataset](https://huggingface.co/datasets/wikipedia/viewer/20220301.simple/train). Our goal is to generate Wikipedia-sounding articles that sound novel but also believable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWMD0fOF5Zuh"
      },
      "source": [
        "- Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWVmDdrq5Y9e",
        "outputId": "46b759a1-788b-41ec-fe35-4250c1536398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset structure is DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'url', 'title', 'text'],\n",
            "        num_rows: 205328\n",
            "    })\n",
            "})\n",
            "an example of a training sequence is April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May. It is one of four months to have 30 days.\n",
            "\n",
            "April always begins on the same day of week as July, and additionally, January in leap years. April always ends on the same day of the week as December.\n",
            "\n",
            "April's flowers are the Sweet Pea and Daisy. Its birthstone is the diamond. The meaning of the diamond is innocence.\n",
            "\n",
            "The Month \n",
            "\n",
            "April comes between March and May, making it the fourth month of the year. It also comes first in the year out of the four months that have 30 days, as June, September and November are later in the year.\n",
            "\n",
            "April begins on the same day of the week as July every year and on the same day of the week as January in leap years. April ends on the same day of the week as December every year, as each other's last days are exactly 35 weeks (245 days) apart.\n",
            "\n",
            "In common years, April starts on the same day of the week as October of the previous year, and in leap years, May of the previous year. In common years, April finishes on the same day of the week as July of the previous year, and in leap years, February and October of the previous year. In common years immediately after other common years, April starts on the same day of the week as January of the previous year, and in leap years and years immediately after that, April finishes on the same day of the week as January of the previous year.\n",
            "\n",
            "In years immediately before common years, April starts on the same day of the week as September and December of the following year, and in years immediately before leap years, June of the following year. In years immediately before common years, April finishes on the same day of the week as September of the following year, and in years immediately before leap years, March and June of the following year.\n",
            "\n",
            "April is a spring month in the Northern Hemisphere and an autumn/fall month in the Southern Hemisphere. In each hemisphere, it is the seasonal equivalent of October in the other.\n",
            "\n",
            "It is unclear as to where April got its name. A common theory is that it comes from the Latin word \"aperire\", meaning \"to open\", referring to flowers opening in spring. Another theory is that the name could come from Aphrodite, the Greek goddess of love. It was originally the second month in the old Roman Calendar, before the start of the new year was put to January 1.\n",
            "\n",
            "Quite a few festivals are held in this month. In many Southeast Asian cultures, new year is celebrated in this month (including Songkran). In Western Christianity, Easter can be celebrated on a Sunday between March 22 and April 25. In Orthodox Christianity, it can fall between April 4 and May 8. At the end of the month, Central and Northern European cultures celebrate Walpurgis Night on April 30, marking the transition from winter into summer.\n",
            "\n",
            "April in poetry \n",
            "Poets use April to mean the end of winter. For example: April showers bring May flowers.\n",
            "\n",
            "Events in April\n",
            "\n",
            "Fixed Events \n",
            "\n",
            " April 1 - April Fools' Day\n",
            " April 1 - Islamic Republic Day (Iran)\n",
            " April 2 - International Children's Book Day\n",
            " April 2 - Thai Heritage and Conservation Day\n",
            " April 2 - World Autism Awareness Day\n",
            " April 2 - Malvinas Day (Argentina)\n",
            " April 4 - Independence Day (Senegal)\n",
            " April 4 - International Day for Landmine Awareness and Assistance\n",
            " April 4 - Peace Day (Angola)\n",
            " April 5 - End of Tax Year (United Kingdom)\n",
            " April 6 - Tartan Day (Canada and United States)\n",
            " April 6 - Chakri Day (Thailand)\n",
            " April 7 - Day of Maternity and Beauty (Armenia)\n",
            " April 7 - Genocide Memorial Day (Rwanda)\n",
            " April 7 - World Health Day\n",
            " April 7 - Women's Day (Mozambique)\n",
            " April 8 - Buddha's Birthday (Buddhism)\n",
            " April 9 - Martyrs' Day (Tunisia)\n",
            " April 9 - Day of National Unity (Georgia)\n",
            " April 9 - Day of the Finnish language\n",
            " April 12 - Cosmonauts' Day (Russia), marking the day of Yuri Gagarin's space flight\n",
            " April 13 - Songkan (Laos), local New Year celebration\n",
            " April 13 - Cambodian New Year\n",
            " April 13 - Thomas Jefferson's Birthday (United States)\n",
            " April 14 - Southeast Asian New Year festivals, including Songkran\n",
            " April 14 - Georgian language Day\n",
            " April 14 - Youth Day (Angola)\n",
            " April 14 - Ambedkar Tayanti (India)\n",
            " April 14 - Pan-American Day\n",
            " April 15 - Tax Day (United States)\n",
            " April 15 - Kim Il-Sung's Birthday (North Korea)\n",
            " April 15 - Father Damien Day (Hawaii)\n",
            " April 15 - Jackie Robinson Day (Major League Baseball)\n",
            " April 16 - Birthday of Queen Margrethe II of Denmark\n",
            " April 16 - Emancipation Day (Washington, DC)\n",
            " April 16 - World Voice Day\n",
            " April 16 - Selena Day (Texas)\n",
            " April 17 - National Day of Syria\n",
            " April 17 - Flag Day (American Samoa)\n",
            " April 17 - Women's Day (Gabon)\n",
            " April 17 - World Hemophilia Day\n",
            " April 18 - Independence Day (Zimbabwe)\n",
            " April 18 - Invention Day (Japan)\n",
            " April 18 - International Day of Monuments and Sites\n",
            " April 19 - Bicycle Day\n",
            " April 19 - Dutch-American Friendship Day\n",
            " April 19 - Birthday of King Mswati III of Swaziland\n",
            " April 19 - Patriots' Day (Massachusetts, Maine, Wisconsin)\n",
            " April 20 - 4/20 in Cannabis Culture\n",
            " April 21 - John Muir Day (California)\n",
            " April 21 - San Jacinto Day (Texas)\n",
            " April 21 - Kartini Day (Indonesia)\n",
            " April 21 - National Tree Planting Day (Kenya)\n",
            " April 21 - First Day of Ridran (Baha'i faith)\n",
            " April 21 - Grounation Day (Rastafari movement)\n",
            " April 22 - Earth Day\n",
            " April 22 - Discovery Day (Brazil)\n",
            " April 23 - Saint George's Day, celebrating the patron saint of several countries, regions and cities (including England and Catalonia)\n",
            " April 23 - World Book Day\n",
            " April 23 - National Sovereignty and Children's Day (Turkey)\n",
            " April 24 - Democracy Day (Nepal)\n",
            " April 24 - Genocide Day (Armenia)\n",
            " April 24 - Republic Day (the Gambia)\n",
            " April 25 - Australia and New Zealand celebrate ANZAC Day. ANZAC  means Australian and New Zealand Army Corps, and began in 1915.\n",
            " April 25 - World DNA Day\n",
            " April 25 - World Malaria Day\n",
            " April 25 - Flag Day (Swaziland, Faroe Islands)\n",
            " April 25 - Freedom Day (Portugal)\n",
            " April 25 - Liberation Day (Italy)\n",
            " April 25 - Army Day (North Korea)\n",
            " April 26 - Union Day (Tanzania)\n",
            " April 26 - Confederate Memorial Day (Texas, Florida)\n",
            " April 27 - Independence Day (Sierra Leone and Togo)\n",
            " April 27 - Freedom Day (South Africa)\n",
            " April 27 - World Tapir Day\n",
            " April 27 - King's Day (Netherlands) from 2014, birthday of Willem-Alexander of the Netherlands\n",
            " April 28 - Workers Memorial Day\n",
            " April 28 - National Day (Sardinia)\n",
            " April 28 - National Heroes Day (Barbados)\n",
            " April 29 - Showa Day (Japan), birthday of Emperor Hirohito, who died in 1989\n",
            " April 29 - International Dance Day\n",
            " April 30 - Former Queen's Day Holiday in the Netherlands (changed to King's Day, April 27 in 2014), was the birthday of former Queen Juliana of the Netherlands\n",
            " April 30 - Flag Day in Sweden (birthday of King Carl XVI Gustaf of Sweden)\n",
            " April 30 - International Jazz Day\n",
            " April 30 - Walpurgis Night (Central and Northern Europe)\n",
            "\n",
            "Moveable Events \n",
            "\n",
            " Easter-related events in Western Christianity:\n",
            " Palm Sunday (between March 15 and April 18)\n",
            " Maundy Thursday (between March 19 and April 22)\n",
            " Good Friday (between March 20 and April 23)\n",
            " Easter Sunday (between March 22 and April 25)\n",
            " Easter Monday (between March 23 and April 26)\n",
            " Eastern Orthodox Easter falls between April 4 and May 8.\n",
            " Ascension Day (Western Christianity), falls between April 30 and June 3.\n",
            " Jewish Passover - falls in the same week as Western Christianity's Holy Week, which is the week leading up to Easter.\n",
            " Mother's Day (UK) falls between March 1 and April 4.\n",
            " World Snooker Championship (late April, early May)\n",
            " Horse racing - Grand National (UK), Kentucky Derby (United States)\n",
            " Start of Daylight Saving Time - Clocks going forward one hour:\n",
            " Most of Mexico\n",
            " Morocco (Ramadan does not include Daylight Saving Time)\n",
            " End of Daylight Saving Time - Clocks going back one hour:\n",
            " Southeast Australia, and New Zealand\n",
            " Chile\n",
            " Marathon Events in the following cities:\n",
            " Belgrade, Serbia\n",
            " Boston, Massachusetts, United States\n",
            " Brighton, United Kingdom\n",
            " Enschede, Netherlands\n",
            " London, United Kingdom\n",
            " Madrid, Spain\n",
            " Paris, France\n",
            " Rotterdam, Netherlands\n",
            " Utrecht, Netherlands\n",
            " Zurich, Switzerland\n",
            "\n",
            "Selection of Historical Events \n",
            "\n",
            " April 1, 1918 - The Royal Air Force is founded.\n",
            " April 1, 1976 - Apple Inc. is founded.\n",
            " April 1, 1979 - The Islamic Republic of Iran is founded.\n",
            " April 1, 1999 - The territory of Nunavut is created in Northern Canada.\n",
            " April 1, 2001 - The Netherlands introduces same-sex marriage, as the first country to do so.\n",
            " April 2, 1519 - Florida is sighted by a European for the first time.\n",
            " April 2, 1930 - Haile Selassie becomes Emperor of Ethiopia.\n",
            " April 2, 1982 - Start of the Falklands War, as Argentine forces land on the Falkland Islands.\n",
            " April 2, 2005 - Pope John Paul II dies aged 84, after 26-and-a-half years as Pope.\n",
            " April 3, 1973 - The first-ever mobile phone call is placed by Martin Cooper in New York City.\n",
            " April 4, 1721 - Robert Walpole becomes the first Prime Minister of Great Britain.\n",
            " April 4, 1841 - William Henry Harrison dies. He was President of the United States for 31 days, the shortest-ever time in office for a US President.\n",
            " April 4, 1960 - Senegal becomes independent.\n",
            " April 4, 1968 - Assassination of Martin Luther King, Jr. in Memphis, Tennessee.\n",
            " April 5, 1722 - Jacob Roggeveen becomes the first European to land on Easter Island, landing there on Easter Sunday.\n",
            " April 6, 1320 - Scotland's independence is confirmed with the Declaration of Arbroath.\n",
            " April 6, 1830 - The Mormon Church is founded.\n",
            " April 6, 1909 - Robert Peary claims to have been first at the North Pole on this date.\n",
            " April 7, 1994 - The Rwandan Genocide begins.\n",
            " April 9, 1865 - American Civil War: Confederate forces under Robert E. Lee surrender to Union forces.\n",
            " April 9, 1940 - World War II: Denmark and Norway are invaded by Nazi Germany.\n",
            " April 9, 1989 - April 9 tragedy: In Tbilisi, Georgia, a peaceful demonstration for independence is broken up by the Soviet Army, killing 20 people. The country gains independence on this date exactly two years later.\n",
            " April 10, 1815 - Mount Tambora in Indonesia erupts in a huge eruption, affecting the world's climate for at least a year.\n",
            " April 10, 2010 - A plane crash near Smolensk, Russia, kills several people who were important in Poland, including President Lech Kaczynski.\n",
            " April 11, 1814 - Napoleon Bonaparte is exiled to the island of Elba.\n",
            " April 11, 1954 - Said to have been the most boring day of the 20th century.\n",
            " April 12, 1861 - The American Civil War begins at Fort Sumter, Charleston, South Carolina.\n",
            " April 12, 1945 - US President Franklin D. Roosevelt dies, and Harry S. Truman replaces him.\n",
            " April 12, 1961 - Yuri Gagarin becomes the first human to fly into space.\n",
            " April 14, 1865 - US President Abraham Lincoln is shot dead at Ford's Theatre by John Wilkes Booth. Lincoln dies the next day.\n",
            " April 14, 2010 - Qinghai Province, China, is hit by an earthquake, killing tens of thousands of people.\n",
            " April 14, 2010 - The eruption of Eyjafjallajokull in Iceland shuts down air traffic around Europe for a week, due to its ash cloud.\n",
            " April 15, 1912 - The ship RMS Titanic sinks near Newfoundland after hitting an iceberg, resulting in the deaths of many of the people on board.\n",
            " April 16, 1943 - Albert Hofmann discovers LSD's effects.\n",
            " April 17, 1946 - Syria gains full independence from France.\n",
            " April 18, 1906 - 1906 San Francisco earthquake: San Francisco, California, is hit by a big earthquake, resulting in fires that destroy large parts of the city.\n",
            " April 18, 1980 - Zimbabwe gains full independence.\n",
            " April 19, 1897 - The first Boston Marathon is held.\n",
            " April 19, 1971 - Sierra Leone becomes a republic.\n",
            " April 19, 1993 - The siege of the Branch Davidians at Waco, Texas, ends in a fire that kills 82 people.\n",
            " April 19, 1995 - Timothy McVeigh carries out the Oklahoma City bombing, killing 169 people.\n",
            " April 19, 2005 - Joseph Alois Ratzinger becomes Pope Benedict XVI.\n",
            " April 20, 1902 - Marie Curie and Pierre Curie refine Radium.\n",
            " April 20, 2010 - Deepwater Horizon oil spill: A massive fire on the Deepwater Horizon drilling rig in the Gulf of Mexico kills 11 workers and causes a massive oil spill, the worst spill in US history.\n",
            " April 21, 753 BC - Legendary founding date of Rome\n",
            " April 21, 1509 - Henry VIII of England becomes King.\n",
            " April 21, 1908 - Frederick Cook claims to have reached the North Pole on this date.\n",
            " April 22, 1502 - Pedro Alvares Cabral becomes the first European to reach present-day Brazil.\n",
            " April 22, 1970 - Earth Day is observed for the first time.\n",
            " April 23, 1533 - The Church of England declares that Henry VIII of England and Catherine of Aragon are not married.\n",
            " April 24, 1916 - The Easter Rising occurs in Dublin, Ireland.\n",
            " April 24, 1990 - The Hubble Space Telescope is launched on the Space Shuttle Discovery.\n",
            " April 25, 1915 - World War I: In Turkey, the Battle of Gallipoli begins, Australian, French, British and New Zealand forces land at Anzac cove.\n",
            " April 25, 1974 - Portugal's dictatorship is overthrown in a coup, in what is known as the Carnation Revolution.\n",
            " April 26, 1937 - Spanish Civil War: German planes bomb the town of Guernica, Basque Country, later depicted in a painting by Pablo Picasso.\n",
            " April 26, 1964 - Tanganyika and Zanzibar merge to form Tanzania.\n",
            " April 26, 1986 - A reactor explosion occurs at the Chernobyl nuclear plant in present-day Ukraine, with radiation spreading around Europe and the world.\n",
            " April 26/27, 1994 - South Africa holds its first free elections.\n",
            " April 27, 1960 - Togo becomes independent from France.\n",
            " April 27, 1961 - Sierra Leone becomes independent from the United Kingdom.\n",
            " April 28, 1789 - Mutiny on the ship Bounty in the Pacific Ocean, lead by Fletcher Christian.\n",
            " April 28, 1945 - Benito Mussolini is executed by Italian partisans.\n",
            " April 28, 1947 - In Peru, Thor Heyerdahl starts his Kon-Tiki expedition aimed at proving his theory that the Polynesian settlers on the Pacific Ocean's islands came from South America.\n",
            " April 29, 1991 - A cyclone in Bangladesh kills an estimated 138,000 people.\n",
            " April 29, 2011 - The wedding of Prince William, Duke of Cambridge and Catherine, Duchess of Cambridge is broadcast worldwide.\n",
            " April 30, 1789 - George Washington becomes the first President of the United States.\n",
            " April 30, 1803 - The United States purchases (buys) the Louisiana territory from France.\n",
            " April 30, 1945 - Adolf Hitler commits suicide on the same day that the Soviet Army raises the Red Flag on Berlin's Reichstag.\n",
            " April 30, 1952 - The Diary of Anne Frank is published in English.\n",
            " April 30, 1975 - The Vietnam War ends, as North Vietnamese forces take Saigon.\n",
            " April 30, 1980 - Queen Juliana of the Netherlands abdicates the throne, and her daughter becomes Queen Beatrix of the Netherlands. Beatrix later also abdicates, on this day in 2013, in favor of her son, King Willem-Alexander of the Netherlands.\n",
            "\n",
            "Trivia \n",
            "\n",
            " In Western Christianity, there is a bigger likelihood of Easter falling in April than in March.\n",
            " The months around April (March and May) both start with an 'M' in the English language, with an 'A' as the second letter.\n",
            " In the English language, April is the first of three months in-a-row, along with May and June, that is also a female given name.\n",
            " The astrological signs for April are Aries (March 21 to April 20) and Taurus (April 21 to May 20).\n",
            " The sweet pea and daisy are the traditional birth flowers for April.\n",
            " Birthstone for April is the Diamond.\n",
            "April 1 is the only day in April to start within the first quarter of the calendar year.\n",
            " If the months of the year were arranged in alphabetical order in the English language, April would come first.\n",
            " Six current European monarchs were born in April. They are King Philippe of Belgium (April 15), Queen Margrethe II of Denmark (April 16), Henri, Grand Duke of Luxembourg (April 16), Elizabeth II of the United Kingdom and Commonwealth realms (April 21), King Willem-Alexander of the Netherlands (April 27), and King Carl XVI Gustaf of Sweden (April 30).\n",
            "\n",
            "References\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "wikipedia_simple_dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "\n",
        "print(\"dataset structure is\", wikipedia_simple_dataset)\n",
        "\n",
        "print(\"an example of a training sequence is\", wikipedia_simple_dataset[\"train\"][\"text\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdKDU7ao5n9i"
      },
      "source": [
        "- Split the dataset into a training set (the first 300 articles) and a the test set (the last 60 articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_2Kl4fl5p-z"
      },
      "outputs": [],
      "source": [
        "# Check the total number of rows in the dataset\n",
        "total_rows = len(wikipedia_simple_dataset[\"train\"])\n",
        "\n",
        "# Define the indices for splitting the dataset\n",
        "train_end_idx = 300  # The end index for the training set\n",
        "test_start_idx = total_rows - 60  # The start index for the test set\n",
        "\n",
        "# Ensure the dataset has at least 12k rows\n",
        "if total_rows < 360:\n",
        "    raise ValueError(\"The dataset has fewer than 360 rows.\")\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "train_dataset = wikipedia_simple_dataset[\"train\"].select(range(train_end_idx))\n",
        "test_dataset = wikipedia_simple_dataset[\"train\"].select(range(test_start_idx, total_rows))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v4vZaLA25PT"
      },
      "source": [
        "1. **(1 point)** Start from a *pretrained* GPT2 transformer with a context of 512 tokens with padding, such that:\n",
        "  - Print the training and test losses every epoch.\n",
        "  - Save the model that performs best on the **test set** as `best_model`\n",
        "  - Train for 10 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Create the tokenizer and tokenize the dataset"
      ],
      "metadata": {
        "id": "uvgypv9xPb6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_len = 512"
      ],
      "metadata": {
        "id": "XQcpC8QeCc86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYlnFUk2-DyT"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "\n",
        "# Initialize the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token, but we can use the EOS token for padding\n",
        "\n",
        "# Tokenize the train and test datasets\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512,return_tensors=\"pt\")\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Create the model"
      ],
      "metadata": {
        "id": "f6CsE3KGSTkN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRE91bfa-Ewb",
        "outputId": "183e6344-6488-4814-d8cf-10902236f547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  2/750 : < :, Epoch 0.01/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-edb117ad811f>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Get the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1592\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1891\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1892\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2776\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2799\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2801\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2802\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2803\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 326.81 MiB is free. Process 365688 has 14.43 GiB memory in use. Of the allocated memory 13.30 GiB is allocated by PyTorch, and 62.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# Initialize the GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=4,  # adjust based on your GPU memory\n",
        "    per_device_eval_batch_size=4,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",  # the metric to use to compare models\n",
        "    greater_is_better=False,  # we want to minimize loss\n",
        "    num_train_epochs=10,\n",
        "    output_dir=\"./gpt2_wikipedia\",\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "def data_collator(features):\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n",
        "        \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
        "        \"labels\": torch.stack([f[\"input_ids\"] for f in features])  # Adding labels here\n",
        "    }\n",
        "\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Get the best model\n",
        "best_model = GPT2LMHeadModel.from_pretrained(\"./gpt2_wikipedia/checkpoint-best\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Create a perplexity metric and a `compute_metric` function to measure the perplexity."
      ],
      "metadata": {
        "id": "Kq5ojgD0S1Ha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8WdV9AZ_KlV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Train the model"
      ],
      "metadata": {
        "id": "Pu3_DYhvTMZQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N341Rjsd-J0y"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Save the best model to your Google Drive to path `/content/drive/MyDrive/IS883_HW2/best_model_wiki`\n",
        "\n"
      ],
      "metadata": {
        "id": "bL-MPDXxOB-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKtUwgzoTIy2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Now load the model back and assign it to best_model"
      ],
      "metadata": {
        "id": "Zm9gJwavMTb9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbuIS2VO-X1_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EocqT3zA54Ec"
      },
      "source": [
        "2. **(1 point)** Write a function that generates text using `best_model`. This function takes the following parameters:\n",
        "\n",
        "  - *temperature*: has a default value 1.0.\n",
        "  - *max_gen_tokens*: specifies the maximum number of tokens in the generated text. Default value is 40.\n",
        "  - *prefix*: default value `tokenizer.bos_token` (i.e., beginning of sentence token).\n",
        "\n",
        "Each time the function is called, it generates 5 possible unique texts. Also, use sampling to avoid generating identical texts.\n",
        "\n",
        "Use the function and generate some texts with different temperatures and prefixes. Comment on the quality of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaqapc5R8nlm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPJl5QHT7WAq"
      },
      "source": [
        "Call the function here to generate 5 different texts. The texts should not be identical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-VaR48j7fgC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9vmWB717-9g"
      },
      "source": [
        "3. **(1 point)** Calculate the perplexity of `best_model` on the test set.\n",
        "\n",
        "Generally, a perplexity lower than 30 is desired. Have you been able to achieve it? If not, would you expect more hyper-parameter tuning to solve the issue? Elaborate and reflect on your answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2jSQ7zc7uI_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "31J5OdtUZFpD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki_j2ew-7fsd"
      },
      "source": [
        "4. **(1.5 point)** Now, train a new GPT2. This model `model_from_scratch` is identical to `best_model`, except that it is trained **from scratch**.\n",
        "Once done:\n",
        "\n",
        "  - Calculate the perplexity on the test set.\n",
        "  - Generate some texts.\n",
        "  - Which model is better `best_model` or `model_from_scratch`? Justify and reflect on your answers."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model and train."
      ],
      "metadata": {
        "id": "5RsiHbgvNP4h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjCD_uyq3sZF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate perplexity"
      ],
      "metadata": {
        "id": "GO6O_OVzNQ5I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSYdW3PB42Uh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate texts"
      ],
      "metadata": {
        "id": "I7p6qlCWNawz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MtfKfSAQMUcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MUQplolNYu21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete your model and clear `cuda` cache for next experiment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U5GtoKeFHw2f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f5-xwuX1OI76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "b1SPEo1kHwhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUHcTinh5nuA"
      },
      "source": [
        "##**Part 2: Using an language models for translation**\n",
        "\n",
        "Here, you will use an *appropriate* language model of your choice and train it on a dataset that has English-to-French song translations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9vHUlIu6Dse"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"Nicolas-BZRD/Original_Songs_Lyrics_with_French_Translation\")\n",
        "\n",
        "# Define a function to check if either 'original_version' or 'french_version' are None\n",
        "def filter_rows(example):\n",
        "    return example['original_version'] is not None and example['french_version'] is not None\n",
        "\n",
        "# Filter the dataset\n",
        "dataset = dataset.filter(filter_rows)\n",
        "\n",
        "print(\"An example row from this dataset\")\n",
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyGkzA4O7A2F"
      },
      "source": [
        "  - Split the dataset into a training set (the first 300 songs) and a test set (the last 60 songs).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFkxCeoj7HC7"
      },
      "outputs": [],
      "source": [
        "# Check the total number of rows in the dataset\n",
        "total_rows = len(dataset[\"train\"])\n",
        "\n",
        "# Ensure the dataset has at least 22k rows\n",
        "if total_rows < 660:\n",
        "    raise ValueError(\"The dataset has fewer than 360 rows.\")\n",
        "\n",
        "# Define the indices for splitting the dataset\n",
        "train_end_idx = 300  # The end index for the training set\n",
        "test_start_idx = total_rows - 60  # The start index for the test set\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "train_dataset = dataset[\"train\"].select(range(train_end_idx))\n",
        "test_dataset = dataset[\"train\"].select(range(test_start_idx, total_rows))\n",
        "\n",
        "# Print the number of rows in training and test sets\n",
        "print(f\"Number of rows in training set: {len(train_dataset)}\")\n",
        "print(f\"Number of rows in test set: {len(test_dataset)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRLgeakE6n1L"
      },
      "source": [
        "1. **(1.5 point)** Choose a good **pre-trained** model for this task. Explain your criteria for choosing this model. It is highly recommended to select one from [HuggingFace official pre-trained models](https://huggingface.co/docs/transformers/index) or [HuggingFace user pre-trained models](https://huggingface.co/models)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the tokenizer. Use a `max_length` of 512. Remove all columns unnecessary for the translation.\n"
      ],
      "metadata": {
        "id": "YUhKjvcGax2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=512"
      ],
      "metadata": {
        "id": "fgOleSadDBOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUX7pl9H69CI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model"
      ],
      "metadata": {
        "id": "6xNjzT4ta5qt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7eHqT67ta6yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model.\n",
        "\n",
        "  2. **(0.5 point)** You might find that your notebook runs out of memory or takes too long to train. What hyper-parameter could you change to address that?"
      ],
      "metadata": {
        "id": "VzYcAFQx7pD5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_EaMnWYUSVoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUjSUgpT9dRk"
      },
      "source": [
        "3. **(1 point)** Translate the following two sentences. Would your model make a good English-to-French translator? Justify your answer.\n",
        "\n",
        "  - \"Just let me hear some of that rock and roll music\"\n",
        "  - \"If you wanna dance with me\\nI've got no kick against modern jazz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKJ9VoliEZ5p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fE9Tt2i6IPHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **(0.5 point)** What would be a good metric for measuring the performance of this model? Could you calculate it for this pair of model and dataset? If yes, show your results and discuss them. If no, elaborate on the reason and how you would go about solving it."
      ],
      "metadata": {
        "id": "dIirjmbSjYOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7lL7c7JhJiO7"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}